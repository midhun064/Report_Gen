To implement a real-time streaming response for your chatbot API, you need to set up your backend to stream partial chunks of the response as they are generated and your frontend to handle this stream and update the UI live.

Here's a simple complete example demonstrating real-time streaming response using:

Python Flask backend streaming partial responses

JavaScript frontend reading the streamed data incrementally as it arrives

Backend (Python Flask) — Real-Time Streaming Response
python
from flask import Flask, request, Response, stream_with_context
import time

app = Flask(__name__)

def generate_real_time_response(user_message):
    # Simulated streaming chunks from AI or process
    simulated_chunks = [
        "Streaming ",
        "your ",
        "chatbot ",
        "response ",
        "in ",
        "real-time."
    ]
    for chunk in simulated_chunks:
        yield f"data: {chunk}\n\n"  # SSE format, can omit if using plain HTTP streaming
        time.sleep(0.7)             # simulate delay for chunk generation

@app.route('/stream-chat', methods=['POST'])
def stream_chat():
    data = request.json
    user_message = data.get('message', '')
    
    return Response(stream_with_context(generate_real_time_response(user_message)),
                    mimetype='text/event-stream')

if __name__ == '__main__':
    app.run(debug=True, threaded=True)
Frontend (HTML + JavaScript) — Consume and Display Streaming Response
xml
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <title>Real-Time Streaming Chatbot</title>
</head>
<body>
    <h2>Chatbot with Real-Time Streaming Response</h2>

    <form id="chatForm">
        <input type="text" id="userInput" placeholder="Type your message here" required />
        <button type="submit">Send</button>
    </form>

    <div id="chatOutput" style="white-space: pre-wrap; margin-top: 20px; border: 1px solid #ddd; padding: 10px; height: 200px; overflow-y: auto;"></div>

    <script>
        const chatForm = document.getElementById('chatForm');
        const chatOutput = document.getElementById('chatOutput');

        chatForm.addEventListener('submit', async (event) => {
            event.preventDefault();

            chatOutput.textContent = ""; // clear previous output
            const userMessage = document.getElementById('userInput').value;

            const response = await fetch('/stream-chat', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ message: userMessage })
            });

            const reader = response.body.getReader();
            const decoder = new TextDecoder('utf-8');
            let done = false;

            while (!done) {
                const { value, done: doneReading } = await reader.read();
                done = doneReading;
                if (value) {
                    const chunk = decoder.decode(value);
                    chatOutput.textContent += chunk;
                    chatOutput.scrollTop = chatOutput.scrollHeight;  // auto scroll down
                }
            }
        });
    </script>
</body>
</html>
Explanation:
The backend stream generator yields chunks of text with a small delay, emulating real-time AI token generation.

The response uses MIME type text/event-stream (Server-Sent Events), but here the frontend reads the raw streaming response with Fetch API’s ReadableStream.

The frontend reads the streamed response chunk-by-chunk and appends it to the chat output dynamically, showing the answer as soon as each partial response arrives.

This setup provides a near real-time streaming interaction like ChatGPT.

